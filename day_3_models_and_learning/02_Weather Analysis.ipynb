{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Some Analysis of the NOAA weather dataset ###\n",
    "\n",
    "In this notebook we are analyzing a sample out of data that was downloaded from http://www1.ncdc.noaa.gov/pub/data/ghcn/daily/, the main file is ghcnd_all.tar.gz which is about 2.4 GB which becomes around 20GB when uncompressed.\n",
    "\n",
    "The data contains about 1 million station-year recordings. That is too much to analyzer on single core machine, so we start by taking a sample of 20,000 recordings of the maximal daily temperatures for a period of a 365 days starting on January 1st (the last day of leap years is discarded)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Checking the versions of some important packages ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "print 'pandas version: ',pd.__version__,'should be at least 0.14.0'\n",
    "print 'numpy version:',np.__version__,'should be at least 1.8.1'\n",
    "print 'sklearn version:',sk.__version__,'should be at least 0.14.1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Switch to the data directory and check it's contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%cd ../data/weather/\n",
    "#%cs ~/data/weather\n",
    "!ls -lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!cat data-source.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- *data-source.txt* - information about downloading the data from NOAA\n",
    "- *ghcnd-readme.txt* - A readme file describing the content of all of the files from ghcnd, in particular:\n",
    "- *ghcnd-stations.txt* - information about each of the meteorological stations.\n",
    "- *Sample_TMAX_By_Year* - a file with 10,000 randomly selected one-year-long TMAX measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!head -1 SAMPLE_TMAX.csv\n",
    "# a Typical single line in the data file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### read data into a Pandas Dataframe ##\n",
    "* Read the data into a DataFrame\n",
    "* Read the data vectors in G\n",
    "* Divide by 10.0 to get the temperatude in degrees celsius\n",
    "* Replace values outside the range [-400,500]  ([-40,50] degrees celsius) with nan  \n",
    "* Paste fixed matrix back into Dout\n",
    "* Show the first few lines of DDout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "header=['station','measurement','year']+range(1,366)\n",
    "# D=pandas.DataFrame(columns=header)\n",
    "Data = pd.read_csv('SAMPLE_TMAX.csv',header=None,names=header)\n",
    "G=Data.ix[:,1:365]\n",
    "G[G<-400]=np.nan   #remove outliers\n",
    "G[G>500]=np.nan\n",
    "G=G/10   # temp / 10 is the temp in celsius\n",
    "Data.ix[:,1:365]=G\n",
    "G=G.transpose()\n",
    "Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "tmp=G.ix[:,:].unstack()\n",
    "print tmp.shape, type(tmp)\n",
    "tmp.hist(bins=50);\n",
    "title('overall distribution of temperatures')\n",
    "print tmp.min(),tmp.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Script for plotting yearly plots ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "dates=[date.fromordinal(i) for i in range(1,366)]\n",
    "def YearlyPlots(T,ttl='',size=(10,7)):\n",
    "    fig=plt.figure(1,figsize=size,dpi=300)\n",
    "    #fig, ax = plt.subplots(1)\n",
    "    if shape(T)[0] != 365:\n",
    "        raise ValueError(\"First dimension of T should be 365. Shape(T)=\"+str(shape(T)))\n",
    "    plot(dates,T);\n",
    "    # rotate and align the tick labels so they look better\n",
    "    fig.autofmt_xdate()\n",
    "    ylabel('temperature')\n",
    "    grid()\n",
    "    title(ttl)\n",
    "YearlyPlots(Data.ix[20:30,1:365].transpose(),ttl='A sample of yearly plots')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Plots for sydney, Australia ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sydneyStations=['ASN00066' in station for station in Data['station']]\n",
    "print Data[sydneyStations]['station'].values\n",
    "#for station in sydneyStations:\n",
    "#    print station,sum(Data['station']==station)\n",
    "tmp=Data[sydneyStations].transpose()\n",
    "YearlyPlots(tmp.ix[1:365,:],ttl='Sydney Stations')\n",
    "#tmp.ix[:,tmp.columns[7]]\n",
    "#Data[sydneyStations][['station','year']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Computing mean and std for each station/year ###\n",
    "And calculating the standard deviation. In this case we are not divi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# a simple scale function to normalize the data-frame row-by-row\n",
    "from numpy import mean, std\n",
    "def scale_temps(Din):\n",
    "    matrix=Din.iloc[:,3:]\n",
    "    Dout=Din.loc[:,['station','year']+range(1,366)]\n",
    "    Mean=mean(matrix, axis=1).values\n",
    "    Dout['Mean']=Mean\n",
    "    Std= std(matrix, axis=1).values\n",
    "    Dout['Std']=Std\n",
    "    # Decided not to normalize each year to have mean zero and std 1\n",
    "    # tmp = pd.DataFrame((matrix.values - Mean[:,np.newaxis])/Std[:,newaxis],columns=range(1,366))\n",
    "    # print tmp.head()\n",
    "    Dout.loc[:,1:365]=matrix.values\n",
    "    return Dout\n",
    "Dout=scale_temps(Data)\n",
    "#reorder the columns\n",
    "Dout=Dout[['station','year','Mean','Std']+range(1,366)]\n",
    "Dout.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Compute average temperature for each day of the year. ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Mean=mean(Dout.ix[:,1:365], axis=0)\n",
    "YearlyPlots(Mean,ttl='The global mean temperature for each day of the year')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "##Principle Component Analysis##\n",
    "An explanation of the principle component analysis (PCA) can be found\n",
    "in [Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis).\n",
    "The site [setosa.io](http://setosa.io/ev/principal-component-analysis/) contains a nice interactive visualization of PCA.\n",
    "\n",
    "In this section we use PCA to model the main ways in which temperarature along the year varies between stations.   We start by cleaning the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Missing Values ###\n",
    "We find the distribution of missing values and decide how to deal with them. From the analysis below we see that most rows have some\n",
    "missing values. We therefor choose to perform the average more carefully, rather than discard rows with many missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "nan_per_row=sum(isnan(Dout.ix[:,1:365]),axis=1)\n",
    "nan_per_row.hist(bins=100)\n",
    "sum(nan_per_row>50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### NaN-tolerant averaging  ###\n",
    "We compute the empirical covariance matrix in a way that tolerates NaN values.\n",
    "\n",
    "<span style=\"color:red\"> In the code below I remove all rows that have a nan in them. If you remve the command **M.dropna(...** then all rows are used. Can you get better results without removing the rows? </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# demonstrating the use of the cell magic %%time, which measures the run-time of the cell.\n",
    "M=Dout.loc[:,1:365]\n",
    "M=M.dropna(axis=0)\n",
    "(columns,rows)=shape(M)\n",
    "Mean=mean(M, axis=0).values\n",
    "\n",
    "print (columns,rows), shape(Mean)\n",
    "C=np.zeros([columns,columns])   # Sum\n",
    "N=np.zeros([columns,columns])   # Counter of non-nan entries\n",
    "print shape(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.fit(M)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "shape(pca.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Percentage of variance Explained ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cumulative_explained = cumsum(pca.explained_variance_ratio_) \n",
    "plot(cumulative_explained);\n",
    "xlim([0,365])\n",
    "grid()\n",
    "\n",
    "print pca.explained_variance_ratio_[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "k=6 # number of components to show.\n",
    "Tcomp=np.transpose(pca.components_)\n",
    "YearlyPlots(Tcomp[:,:k],ttl='The most significant eigen-vectors')\n",
    "legend(range(0,k));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "k=50\n",
    "Eig=np.matrix(Tcomp[:,:k])\n",
    "#print [np.linalg.norm(U[:,i]) for i in range(k)]\n",
    "matrix=np.matrix(Dout.ix[:,1:365])-Mean\n",
    "matrix[isnan(matrix)]=0\n",
    "print shape(Eig),shape(matrix)\n",
    "Prod=matrix*Eig;\n",
    "print shape(Prod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Insert coefficients for k top eigenvectors into the dataframe **Dout**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(k-1,-1,-1):\n",
    "    Ser=pd.Series(np.array(Prod)[:,i],index=Dout.index)\n",
    "    Dout.insert(4,'V'+str(i),Ser)\n",
    "Dout.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Longitude,Latitude information ###\n",
    "Loading the station longitude/latitude and merging it into the Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!cat ghcnd-readme.txt   # uncomment to read the readme file."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "------------------------------\n",
    "Variable   Columns   Type\n",
    "------------------------------\n",
    "ID            1-11   Character\n",
    "LATITUDE     13-20   Real\n",
    "LONGITUDE    22-30   Real\n",
    "ELEVATION    32-37   Real\n",
    "STATE        39-40   Character\n",
    "NAME         42-71   Character\n",
    "GSNFLAG      73-75   Character\n",
    "HCNFLAG      77-79   Character\n",
    "WMOID        81-85   Character\n",
    "------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Make all lines be of length 90 to solve problem wilth read_fwf\n",
    "out=open('ghcnd-stations_buffered.txt','w')\n",
    "for line in open('ghcnd-stations.txt','r').readlines():\n",
    "    line=line.rstrip()\n",
    "    string=line+' '*(90-len(line))+'\\n'\n",
    "    out.write(string)\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A grid to define the character locations of the fields\n",
    "\n",
    "AE000041196  25.3330   55.5170   34.0    SHARJAH INTER. AIRP            GSN     41196\n",
    "0123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890\n",
    "0         1         2         3         4         5         6         7         8         9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "colspecs = [(0, 11), (11, 21), (21, 31), (31, 38),(39,41),(41,72),(72,76),(76,80),(80,86)]\n",
    "stations = pd.read_fwf('ghcnd-stations_buffered.txt', colspecs=colspecs, header=None, index_col=0,\n",
    "                       names=['latitude','longitude','elevation','state','name','GSNFLAG','HCNFLAG','WMOID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#stations['elevation'][stations['elevation']==-999.9]=0  # decided not to remove -999.9 because this confuses hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "stations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### perform a **JOIN** ###\n",
    "Join the geographical information into **Dout**, creating a new dataframe called **Djoined**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Djoined=Dout.join(stations,on='station')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Djoined.columns[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "Djoined['AbsLatitude']=abs(Djoined['latitude'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "Djoined.ix[:5,['station',u'longitude','latitude',u'AbsLatitude','Mean','Std','V0','V1','V2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Looking for significant correlations and dependencies ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "Djoined[['latitude','elevation','Mean','Std','V0','V1','V2','V3','V4','V5']].cov()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<span style=\"color:red\"> The correlations between different $V_i$ components should be zero, which it isn't.\n",
    "Is this due to numerical roundoff errors? Are the correlations statistically significant for this sample size? </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "Djoined[['latitude','elevation','Mean','Std','V0','V1','V2','V3','V4','V5']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Choosing significance threshold so that none of the correlations between the Vi-s are significant.\n",
    "abs(Djoined[['latitude','elevation','Mean','Std','V0','V1','V2','V3','V4','V5']].corr())>0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Seems to take enormous amount of time\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "df = Djoined.ix[:,['latitude','elevation','Mean','Std','V0','V1','V2','V3','V4','V5']]\n",
    "scatter_matrix(df, alpha=0.03, figsize=(20, 20), diagonal='kde');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X='latitude'\n",
    "Djoined.ix[:,X].hist(bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X='latitude';Y='Mean'\n",
    "scatter(Djoined.loc[:,X],Djoined.loc[:,Y],alpha=0.05)\n",
    "xlabel(X)\n",
    "ylabel(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#checking for an anomaly in the elevations of stations\n",
    "Djoined[['station','elevation']][Djoined['elevation']<-500].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!grep ASN00010865 ghcnd-stations.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Plotting maps ###\n",
    "Working through http://matplotlib.org/basemap/\n",
    "\n",
    "* http://en.wikipedia.org/wiki/Map_projection\n",
    "* http://matplotlib.org/basemap/users/mapsetup.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "lons=stations.ix[:,'longitude'].values\n",
    "lats=stations.ix[:,'latitude'].values\n",
    "station_names=stations.index.values\n",
    "ll=len(lons)\n",
    "lonmin=-180;lonmax=180;latsmin=-80;latsmax=80;\n",
    "select=(lons>lonmin) * (lons<lonmax)*(lats>latsmin)*(lats<latsmax)\n",
    "print sum(select)\n",
    "station_names=station_names[select]\n",
    "lons=lons[select]\n",
    "lats=lats[select]\n",
    "print len(lons),len(lats),len(station_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# http://matplotlib.org/basemap/users/merc.html\n",
    "\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# llcrnrlat,llcrnrlon,urcrnrlat,urcrnrlon\n",
    "# are the lat/lon values of the lower left and upper right corners\n",
    "# of the map.\n",
    "# lat_ts is the latitude of true scale.\n",
    "# resolution = 'i' means use intermediate resolution coastlines.\n",
    "plt.figure(figsize=(15,10),dpi=300)\n",
    "m = Basemap(projection='merc',llcrnrlat=latsmin,urcrnrlat=latsmax,\\\n",
    "            llcrnrlon=lonmin,urcrnrlon=lonmax,lat_ts=20,resolution='i')\n",
    "m.drawcoastlines()\n",
    "m.fillcontinents(color='coral',lake_color='aqua')\n",
    "\n",
    "# draw parallels and meridians.\n",
    "parallels = np.arange(-80,81,10.)\n",
    "# labels = [left,right,top,bottom]\n",
    "m.drawparallels(parallels,labels=[False,True,True,False])\n",
    "meridians = np.arange(10.,351.,20.)\n",
    "m.drawmeridians(meridians,labels=[True,False,False,True])\n",
    "\n",
    "#m.drawparallels(np.arange(-90.,91.,30.))\n",
    "#m.drawmeridians(np.arange(-180.,181.,60.))\n",
    "m.drawmapboundary(fill_color='aqua')\n",
    "\n",
    "# draw map with markers for locations\n",
    "x, y = m(lons,lats)\n",
    "m.plot(x,y,'g.')\n",
    "\n",
    "plt.title('weather stations')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get to these coordinate on Google Maps, type the latitude and longitude in decimal in the search box or use:\n",
    "https://www.google.com/maps/place/72%C2%B018'00.0%22S+170%C2%B013'00.1%22E/@-72.3,170.216694,17z/data=!3m1!4b1!4m2!3m1!1s0x0:0x0\n",
    "\n",
    "<span style=\"color:red\">HW questions</span>\n",
    "\n",
    "1. Waiting for somebody to write a script that will do that automatically from python\n",
    "2. Can you create a map where the denity of points is represented as a density map (topographical map)?\n",
    "3. Can you create a map that would represent, using color, the values of a chosen column (Mean, Std, V0,V1 etc.)? What conclusions can you draw from this map?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reconstruction ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Sachin, this function crashes, can you find out why and fix it?\n",
    "# It ran fine last year.\n",
    "%matplotlib inline\n",
    "def plot_reconstructions(selection,rows=2,columns=7):\n",
    "    Recon=Eig*Prod.transpose()+Mean[:,np.newaxis]\n",
    "    plt.figure(figsize=(columns*3,rows*3),dpi=300)\n",
    "    j=0;\n",
    "    for i in selection:\n",
    "        subplot(rows,columns,j); \n",
    "        j += 1; \n",
    "        if j>=rows*columns: break\n",
    "        print i,j,shape(Recon)\n",
    "        plot(Recon[:,i])\n",
    "        plot(Djoined.ix[i,1:365]);\n",
    "        title(Djoined.ix[i,'station']+' / '+str(Djoined.ix[i,'year']))\n",
    "        xlim([0,365])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Observe in the reconstructions below that the bloue line fills in (extrapolation/interpolation) the places where the measurements are not available. It also reduces the fluctuations in the relative to the original line. Recall the we are using the k top eigenvectors which explain about 88% of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_reconstructions(range(50,51),rows=7,columns=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> Check how the approximations change/improve as you increase the number of coefficients</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "hist(Djoined.ix[:,'V0'],bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "selection= [i for i in range(shape(Djoined)[0]) if Djoined.ix[i,'latitude']<-10]\n",
    "plot_reconstructions(selection,rows=7,columns=3)\n",
    "shape(selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Can you reduce the reconstruction error (using a fixed number of eigenvectors) by splitting the stations according to region (for example country, state, latitudal range). Note that having a regions with very few readings defeats the purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
