{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Lecture3_Exercises\n",
    "\n",
    "**Economics dataset analysis**\n",
    "\n",
    "The folder economics (~/DSE200/DSE200/data/economics/) has a set of small datasets corresponding to different economics topics. We will use Pandas to incorporate these datasets into our workflow, merge them and analyze the economic trends as a function of time.\n",
    "\n",
    "To achieve this, do the following\n",
    "\n",
    "1. Read each of the csv files iteratively\n",
    "2. Merge all of the data into a single dataframe by building a dictionary where the keys are the codes and the values are the Series from each downloaded file\n",
    "3. Construct the Term and Default premia series using basic math on the series, and mmerge the resulting series using JOIN operation. HINT: term_premium = merged_data[’GS10’] - merged_data[’GS1’] and default_premium = merged_data[’BAA’] - merged_data[’AAA’]\n",
    "4. Process the data\n",
    "5. Plot ’GDP_growth’,’IP_growth’ and 'Unemp_rate' a a function of time and draw inferences\n",
    "6. Use pandas function scatter_matrix to generate scatter plots of ’GDP_growth’,’IP_growth’ and 'Unemp_rate' in a mmatrix form with kernel density plots along the diagonals.\n",
    "\n",
    "Note: Processing the data is of utmost importance for better readability and understanding of the data. Process the above data by ensuring the following\n",
    "\n",
    "1. dropping the rows with null values\n",
    "2. Output data regularly to see if they are following regular format. Use pandas.series.pct_change wherever necessary\n",
    "\n",
    "**The codes and their corresponding series representation**\n",
    "\n",
    "                            Series                   Code           Frequency\n",
    "                            Real GDP                 GDPC1          Quarterly\n",
    "                            Industrial Production   INDPRO          Quarterly\n",
    "                            Core CPI               CPILFESL         Monthly\n",
    "                            Unemployment Rate       UNRATE          Monthly\n",
    "                            10 Year Yield            GS10           Monthly\n",
    "                            1 Year Yield             GS1            Monthly\n",
    "                            Baa Yield                BAA            Monthly\n",
    "                            Aaa Yield                AAA            Monthly\n",
    "                            \n",
    "** Variable Description **\n",
    "\n",
    "                            Series                        Description\n",
    "                            Treated                       Dummy indicating whether the candidate received the treatment\n",
    "                            Age                           Age in years\n",
    "                            Education (years)             Years of Education\n",
    "                            Black                         Dummy indicating African-American\n",
    "                            Hispanic                      Dummy indicating Hispanic\n",
    "                            Married                       Dummy indicating married\n",
    "                            Real income Before ($)        Income before program\n",
    "                        Real income After ($)         Income after program           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nutrition dataset analysis**\n",
    "\n",
    "Download the dataset from http://ashleyw.co.uk/files/foods-2011-10-03.json.zip\n",
    "This data set is a compilation of data provided by the US department of Agriculture. The data set contains data for 6,636 unique foods and 94 unique nutrients (with an average of 56.5 nutrients per food)\n",
    "\n",
    "Do the following\n",
    "\n",
    "Use the built in python json module to load the food data into a python list\n",
    "\n",
    "Your code should look like this\n",
    "\n",
    "> import json\n",
    "> db = json.load(open('foods-2011-10-03.json'))\n",
    "\n",
    "db will be a list of 6636 python dictionaries, each containing nutritional information for a different food item. Each dictionary will have the following keys:\n",
    "\n",
    "    1.portions\n",
    "    2.description\n",
    "    3.tags\n",
    "    4.nutrients\n",
    "    5.group\n",
    "    6.id\n",
    "    7.manufacture\n",
    "\n",
    "Now, create a DataFrame of meta_data using the description, group, id, and manufacturer items in each dictionary.\n",
    "\n",
    "Loop over db and construct a list of DataFrames containing the nutritional information for each record in db. Make sure to add a column to each of these DataFrames that contains the unique food id (id key in the dictionary)\n",
    "\n",
    "Finally, use the pandas combining techniques to create a nutrients DataFrame. After you have done this drop duplicate entries in this DataFrame. For example, if you had named the objects nuts you would do\n",
    "\n",
    "nuts = nuts.drop_duplicates()\n",
    "\n",
    "Use the rename method to make sure that the description and group columns are un-ambiguous for both the meta_data DataFrame and the nutrients DataFrame (These column names are duplicated because every food has a description and group and each nutrient also has those identifiers). \n",
    "\n",
    "Finally, use the data combining routines to come up with a foods DataFrame containing all the meta_data and nutritional information. Make sure to do an outer style merge on the correct columns.\n",
    "\n",
    "Using the foods DataFrame you have been building, compute the following things:\n",
    "\n",
    "1. The food item with the highest content of each nutrient.\n",
    "2. A function that accepts a nutrient name and a quantile value and generates a horizontal bar plot of the amount of that nutrient in each food group. Provide a plot title. HINT: You will need to use the quantile and sort (or order ) methods in order for this to work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classifying Digits**\n",
    "SKLearn has many intereting datasets pre-loaded in it, one of which is load_digits (sklearn.datasets.load_digits - http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html). Load_digits is a dataset of digits, with each datapoint being a 8x8 image of a digit. \n",
    "\n",
    "You can load this dataset as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the images can be visualized as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pylab as pl \n",
    "pl.gray() \n",
    "pl.matshow(digits.images[0]) \n",
    "pl.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Visualize the first 15 digits to get a better understanding of the data\n",
    "2. With x as data and y as target, classify the above datasets into individual targets using a decision tree\n",
    "3. Perform this classification task with sklearn.svm.SVC. How does the choice of kernel affect the results?\n",
    "4. Perform this classification task with sklearn.ensemble.RandomForestClassifier. Write in the markdown below the impact each of the parammeter had on the result\n",
    "\n",
    "\n",
    "\n",
    "        1. max_depth: \n",
    "        2. max_features:\n",
    "        3. n_estimators:\n",
    "\n",
    "\n",
    "\n",
    "Try a few sets of parameters for each model and check the F1 score (sklearn.metrics.f1_score) on your results. Output the best F1 score that you achieve. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dimensionality Reduction - PCA**\n",
    "\n",
    "Principal Component Analysis is a very powerful unsupervised method for dimensionality reduction in data. \n",
    "\n",
    "Apply dimensionality reduction technique PCA (Principle Component Analysis) on the prev dataset - load_digits(). \n",
    "Use the sklearn inbuilt tool sklearn.decomposition.PCA\n",
    "\n",
    "Print the shape of the matrix before and after the application of PCA on the dataset. Using variance, analyze the ammount of information thrown away and plot the variance(cumulative) against the number of components  \n",
    "\n",
    "Try other dimensionality reduction techniques - sklearn.decomposition.RandomizedPCA, sklearn.decomposition.FastICA  as well \n",
    "(this extended analysis need not be submitted as part of homework)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
